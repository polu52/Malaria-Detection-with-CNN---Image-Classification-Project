{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf04eb-5180-4a12-8e0a-f1a994c5b2a7",
   "metadata": {},
   "source": [
    "# Image Classification with CNN for Malaria Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e0478-6a1b-49da-9e8c-dd7f2881e2f0",
   "metadata": {},
   "source": [
    "In this project, we will create a classification model using malaria images labeled as infected and uninfected, obtained from Kaggle.com.We will use Keras deep learning. With this model, when a new malaria cell image is provided, the machine will automatically determine whether it is infected or uninfected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c32656-13a5-4309-ab09-a6b2592c70f2",
   "metadata": {},
   "source": [
    "<img src='https://eu.biogents.com/wp-content/uploads/malaria-transmission-cycle-without-headline-72dpi.jpg' >\n",
    "\n",
    "<a href='https://www.kaggle.com/datasets/iarunava/cell-images-for-detecting-malaria/data'> Click to Reach Malaria Data <a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995a09a-e4cb-4687-90a6-507707117277",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ae7235-4230-4a58-8be9-adf3760fae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81641498-3cdb-48b7-9e05-c75910b2e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Infected','Uninfected']\n",
    "img_path='archive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2cf1f4f-2c54-4515-b9d2-24f5d9fe9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list=[]\n",
    "label_list=[]\n",
    "for label in labels:\n",
    "    for img_file in os.listdir(img_path+label):\n",
    "        img_list.append(img_path+label+'/'+img_file)\n",
    "        label_list.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4f338d-6cae-4037-8ebb-b92e1b999969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'img':img_list,'label':label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5696102a-50fd-4aae-9934-e225195308db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>archive/Infected/C100P61ThinF_IMG_20150918_144...</td>\n",
       "      <td>Infected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>archive/Infected/C100P61ThinF_IMG_20150918_144...</td>\n",
       "      <td>Infected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>archive/Infected/C100P61ThinF_IMG_20150918_144...</td>\n",
       "      <td>Infected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>archive/Infected/C100P61ThinF_IMG_20150918_144...</td>\n",
       "      <td>Infected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>archive/Infected/C100P61ThinF_IMG_20150918_144...</td>\n",
       "      <td>Infected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 img     label\n",
       "0  archive/Infected/C100P61ThinF_IMG_20150918_144...  Infected\n",
       "1  archive/Infected/C100P61ThinF_IMG_20150918_144...  Infected\n",
       "2  archive/Infected/C100P61ThinF_IMG_20150918_144...  Infected\n",
       "3  archive/Infected/C100P61ThinF_IMG_20150918_144...  Infected\n",
       "4  archive/Infected/C100P61ThinF_IMG_20150918_144...  Infected"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b730b98b-a90d-4bd0-8b1c-34132caf1444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27553</th>\n",
       "      <td>archive/Uninfected/C99P60ThinF_IMG_20150918_14...</td>\n",
       "      <td>Uninfected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27554</th>\n",
       "      <td>archive/Uninfected/C99P60ThinF_IMG_20150918_14...</td>\n",
       "      <td>Uninfected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27555</th>\n",
       "      <td>archive/Uninfected/C99P60ThinF_IMG_20150918_14...</td>\n",
       "      <td>Uninfected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27556</th>\n",
       "      <td>archive/Uninfected/C99P60ThinF_IMG_20150918_14...</td>\n",
       "      <td>Uninfected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27557</th>\n",
       "      <td>archive/Uninfected/C99P60ThinF_IMG_20150918_14...</td>\n",
       "      <td>Uninfected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     img       label\n",
       "27553  archive/Uninfected/C99P60ThinF_IMG_20150918_14...  Uninfected\n",
       "27554  archive/Uninfected/C99P60ThinF_IMG_20150918_14...  Uninfected\n",
       "27555  archive/Uninfected/C99P60ThinF_IMG_20150918_14...  Uninfected\n",
       "27556  archive/Uninfected/C99P60ThinF_IMG_20150918_14...  Uninfected\n",
       "27557  archive/Uninfected/C99P60ThinF_IMG_20150918_14...  Uninfected"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4245cd14-247e-43e4-8b06-aebfb0d9931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Infected':1,'Uninfected':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7aa004-1043-4ad4-86fd-80de7bb54e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encode_label']=df['label'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce94b90-5614-40e8-8a01-661a03b116bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>encode_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25038</th>\n",
       "      <td>archive/Uninfected/C65P26N_ThinF_IMG_20150818_...</td>\n",
       "      <td>Uninfected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>archive/Infected/C105P66ThinF_IMG_20150924_095...</td>\n",
       "      <td>Infected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>archive/Infected/C59P20thinF_IMG_20150803_1134...</td>\n",
       "      <td>Infected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4146</th>\n",
       "      <td>archive/Infected/C140P101ThinF_IMG_20151005_21...</td>\n",
       "      <td>Infected</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22748</th>\n",
       "      <td>archive/Uninfected/C238NThinF_IMG_20151207_115...</td>\n",
       "      <td>Uninfected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     img       label  \\\n",
       "25038  archive/Uninfected/C65P26N_ThinF_IMG_20150818_...  Uninfected   \n",
       "464    archive/Infected/C105P66ThinF_IMG_20150924_095...    Infected   \n",
       "9916   archive/Infected/C59P20thinF_IMG_20150803_1134...    Infected   \n",
       "4146   archive/Infected/C140P101ThinF_IMG_20151005_21...    Infected   \n",
       "22748  archive/Uninfected/C238NThinF_IMG_20151207_115...  Uninfected   \n",
       "\n",
       "       encode_label  \n",
       "25038             0  \n",
       "464               1  \n",
       "9916              1  \n",
       "4146              1  \n",
       "22748             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf602e29-1976-4d59-93cd-da01225c89f0",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb030b3-0402-43ac-a08b-d8ce0bc718b4",
   "metadata": {},
   "source": [
    "We will identify x and y datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb0ca15-cc7e-449c-8737-3c7079407ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for img in df['img']:\n",
    "    img=cv2.imread(img)\n",
    "    img=cv2.resize(img,(32,32)) #We resized the image to 32x32 pixels.\n",
    "    img=img/255.0 #normalize the data\n",
    "    x.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6eb6550-e6f9-453c-8f09-b935f6556235",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f640620-9b73-4d0a-9a42-6f18a1f6c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['encode_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a098ed8-737c-4b56-993a-7e7bdcfd3337",
   "metadata": {},
   "source": [
    "Let's import train test split and we will split%20 of our datas as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac6b324-ce6f-442b-a69f-f07afda0ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9883c8-0fdf-4530-b0d3-bc6388b67aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f353c-6806-40c7-9063-cce4719f4486",
   "metadata": {},
   "source": [
    "CNN - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e8cb908-ffe7-4648-b6c3-b387eec6374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D,Dropout,BatchNormalization,Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90c5d45f-8c8a-4d37-aa15-9c938a2d1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(32,32,3)))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98f5fc88-d2cc-40f8-85ce-e3a4656d2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6142 - loss: 0.6523 - val_accuracy: 0.8500 - val_loss: 0.3247\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2306 - val_accuracy: 0.9321 - val_loss: 0.1803\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.1708 - val_accuracy: 0.9390 - val_loss: 0.1686\n",
      "Epoch 4/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9483 - loss: 0.1562 - val_accuracy: 0.9463 - val_loss: 0.1590\n",
      "Epoch 5/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9522 - loss: 0.1409 - val_accuracy: 0.9447 - val_loss: 0.1659\n",
      "Epoch 6/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9537 - loss: 0.1375 - val_accuracy: 0.9136 - val_loss: 0.2453\n",
      "Epoch 7/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1258 - val_accuracy: 0.9410 - val_loss: 0.1632\n",
      "Epoch 8/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1223 - val_accuracy: 0.9427 - val_loss: 0.1636\n",
      "Epoch 9/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9585 - loss: 0.1189 - val_accuracy: 0.9383 - val_loss: 0.1804\n",
      "Epoch 10/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.9608 - loss: 0.1155 - val_accuracy: 0.9445 - val_loss: 0.1670\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a9ef0d-8d89-4a84-ba78-e431cb8ded09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('my__model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7485e-0430-43f2-bc74-62a4d71ca684",
   "metadata": {},
   "source": [
    "We trained our model and achieved a 96% accuracy score. We saved our model, and now we can use it in any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a204fa-a6b5-4c9c-bb7f-20b8c0b74fc5",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41190f7f-bc1e-45be-a397-af051172e5ae",
   "metadata": {},
   "source": [
    "We trained a new model from scratch using CNN. However, we can also achieve our goal by using pre-trained models and providing them with our own data. Among the pre-trained models, the most widely used in the field of image processing are the ResNet50 and VGG16 models. Here, we will use the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8b3dc6-e80e-455a-b98c-f71d96d246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D,Dropout,BatchNormalization,Reshape\n",
    "from tensorflow.keras.applications import VGG16,ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983da8e7-8c62-49b6-a3ec-d139036e144a",
   "metadata": {},
   "source": [
    "We will reload our data, but this time we will use a more advanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdca9f5-8457-4751-af3d-e57f4d6dfe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22048 images belonging to 2 classes.\n",
      "Found 5510 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polu5\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 63ms/step - accuracy: 0.8225 - loss: 0.3873 - val_accuracy: 0.8641 - val_loss: 0.3129\n",
      "Epoch 2/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 72ms/step - accuracy: 0.8699 - loss: 0.3034 - val_accuracy: 0.8432 - val_loss: 0.3571\n",
      "Epoch 3/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 72ms/step - accuracy: 0.8782 - loss: 0.2871 - val_accuracy: 0.8817 - val_loss: 0.2900\n",
      "Epoch 4/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 72ms/step - accuracy: 0.8743 - loss: 0.2881 - val_accuracy: 0.8599 - val_loss: 0.3232\n",
      "Epoch 5/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 72ms/step - accuracy: 0.8816 - loss: 0.2743 - val_accuracy: 0.8813 - val_loss: 0.2895\n",
      "Epoch 6/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 72ms/step - accuracy: 0.8862 - loss: 0.2672 - val_accuracy: 0.8786 - val_loss: 0.2814\n",
      "Epoch 7/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 72ms/step - accuracy: 0.8875 - loss: 0.2630 - val_accuracy: 0.8799 - val_loss: 0.2829\n",
      "Epoch 8/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 73ms/step - accuracy: 0.8913 - loss: 0.2521 - val_accuracy: 0.8788 - val_loss: 0.2821\n",
      "Epoch 9/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 73ms/step - accuracy: 0.8956 - loss: 0.2484 - val_accuracy: 0.8693 - val_loss: 0.3057\n",
      "Epoch 10/10\n",
      "\u001b[1m689/689\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 73ms/step - accuracy: 0.8923 - loss: 0.2534 - val_accuracy: 0.8748 - val_loss: 0.2918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e32997eab0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='archive'\n",
    "img_width,img_heigth=32,32\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1/255, validation_split=.20)\n",
    "\n",
    "train_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='training')\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='validation')\n",
    "\n",
    "base_model=VGG16(weights='imagenet', input_shape=(img_width,img_heigth,3),include_top=False)\n",
    "model=Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_datagenerator,epochs=10,validation_data=test_datagenerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8711414-3d8b-4739-a0b9-2722eac778d4",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2537c5-8801-4ae3-994f-360aed61705e",
   "metadata": {},
   "source": [
    "We turned the file paths of the malaria cell images obtained from Kaggle.com into a DataFrame with the help of the Pandas and OS libraries. Then, we sequentially read all the images using the OpenCV library and normalized them for easier processing. Afterward, we trained our model using Keras deep learning and achieved a 96% accuracy score. As a second step, we used the VGG16 model by feeding our data into it, training the pre-trained model with our data, and making it suitable for our purpose. Both methods can be used for Image Classification, but since training a model from scratch is a time-consuming process, the common approach is to use a pre-trained model and apply transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56fa90-6dad-4f65-907e-7ac1d58cd4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
